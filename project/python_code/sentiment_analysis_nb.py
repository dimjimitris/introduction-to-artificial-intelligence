# -*- coding: utf-8 -*-
"""sentiment_analysis_NB.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1sLWj7A23bFseVa4d7w-N_OL6tsdtL7y6
"""

import requests
import os

def download_file(url, filename):
    """
    Download a file from a given URL and save it to a given filename.
    If the file already exists, do nothing.
    """
    if not os.path.exists(filename):
        # send a GET request to the URL
        response = requests.get(url)
        # write the content of the response to a file
        with open(filename, 'wb') as file:
            file.write(response.content)

# Download the BERT vocabulary file if not already present
VOCAB_URL = "https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-vocab.txt"
VOCAB_FILE = "bert-base-uncased-vocab.txt"

download_file(VOCAB_URL, VOCAB_FILE)

import nltk
from nltk.corpus import stopwords

# Download necessary NLTK data
nltk.download('stopwords')

import pickle

def save_to_pickle(obj, filename):
    """
    Save an object to a pickle file.
    """
    print(f"Saving {filename} to pickle file...")
    with open(filename, 'wb') as file:
        pickle.dump(obj, file)

def load_from_pickle(filename):
    """
    Load an object from a pickle file if it exists.
    """
    if not os.path.exists(filename):
        return None
    print(f"Loading {filename} from pickle file...")
    with open(filename, 'rb') as file:
        return pickle.load(file)

import bz2
import pandas as pd; print(pd.__version__)
from langdetect import detect, DetectorFactory
from langdetect.lang_detect_exception import LangDetectException
import re
import emoji

DetectorFactory.seed = 42

def preprocess_data(sample_size, filename):
    # Load and balance the dataset
    file_path = 'train.ft.txt.bz2'
    positive_reviews = []
    negative_reviews = []

    # we want to use a balanced dataset of sample reviews
    # we pick half negative and half positive reviews
    with bz2.open(file_path, 'rt', encoding='utf-8') as file:
        for line in file:
            label, text = line.split(' ', 1)
            label = int(label[-1])
            if label == 1 and len(negative_reviews) < sample_size // 2:
                negative_reviews.append([label, text])
            elif label == 2 and len(positive_reviews) < sample_size // 2:
                positive_reviews.append([label, text])
            if len(positive_reviews) == sample_size // 2 and len(negative_reviews) == sample_size // 2:
                break

    # Combine positive and negative reviews
    data = positive_reviews + negative_reviews
    df = pd.DataFrame(data, columns=['label', 'text'])

    # Function to detect language
    def detect_language(text):
        try:
            return detect(text)
        except LangDetectException:
            return "unknown"

    # Detect language
    df['language'] = df['text'].apply(detect_language)

    # Filter out non-English reviews
    df = df[df['language'] == 'en']
    df = df.drop(columns=['language'])

    # Initialize stopwords
    stop_words = set(stopwords.words('english'))

    def clean_text(text):
        # Replace emojis with descriptive words
        text = emoji.demojize(text, delimiters=(" ", " "))
        # Remove HTML tags
        text = re.sub(r'<.*?>', '', text)
        # Remove URLs
        text = re.sub(r'http\S+|www.\S+', '', text)
        # Remove hashtags
        text = re.sub(r'#\w+', '', text)
        # Remove special characters and digits
        text = re.sub(r'[^a-zA-Z\s]', '', text)
        # Convert to lowercase
        text = text.lower()
        # Remove stopwords
        text = ' '.join(word for word in text.split() if word not in stop_words)
        return text

    # Apply text cleaning to the 'text' column
    df['text'] = df['text'].apply(clean_text)

    # this is a rather simple way to shuffle a DataFrame
    df = df.sample(frac=1,random_state=42)

    return df

SAMPLE_SIZE = 20_000

# %% Load and pre-process dataset, then save using pickle

# Define the path for the pickle file
PROCESSED_DATA_FILE = 'processed_data.pkl'

# Check if the processed data already exists
df = load_from_pickle(PROCESSED_DATA_FILE)

# If the processed data does not exist, preprocess the data
if df is None:
    df = preprocess_data(SAMPLE_SIZE, PROCESSED_DATA_FILE)
    save_to_pickle(df, PROCESSED_DATA_FILE)

# Show the first 5 rows of the processed dataset
print(df.head())

import seaborn as sns

sns.countplot(x=df['label'])

import numpy as np; print(np.__version__)

#SLICE_SIZE = 5_000
#df_slice_1 = df[:SLICE_SIZE // 2].copy()
#df_slice_2 = df[df.shape[0] // 2: df.shape[0] // 2 + SLICE_SIZE // 2].copy()
#df_slice_2.index = np.arange(SLICE_SIZE // 2, SLICE_SIZE)
#df_slice = pd.concat([df_slice_1, df_slice_2])
df_slice = df # originally we did some tests with smaller DataFrame sizes. Now this cell is simply an alias for the same DataFrame

df_slice['label'] = df_slice['label'].apply(lambda l: int(l > 1))

sns.countplot(x=df_slice['label'])

tokenized_reviews = df_slice['text'].apply(lambda x: x.split())

from sklearn.feature_extraction.text import CountVectorizer
from nltk.tokenize import RegexpTokenizer

token = RegexpTokenizer(r'[a-zA-Z0-9]+')
cv = CountVectorizer(stop_words='english',ngram_range = (1,1),tokenizer = token.tokenize)

text_counts = cv.fit_transform(df_slice['text'])

from sklearn.model_selection import train_test_split

X = text_counts
y = df_slice['label']

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

from sklearn.naive_bayes import MultinomialNB

MNB = MultinomialNB()
MNB.fit(X_train, y_train)

from sklearn.metrics import (
    accuracy_score,
    classification_report,
    confusion_matrix,
)

y_pred = MNB.predict(X_test)

print(f"Multinomial Naive Bayes (MNB) with ngram_range={cv.ngram_range}")
print(f"Accuracy: {accuracy_score(y_test, y_pred)}")
print("\n")
print(classification_report(y_test, y_pred))
print("\n")
print(confusion_matrix(y_test, y_pred))

# let's try different n-grams
cv = CountVectorizer(stop_words='english',ngram_range = (2,2),tokenizer = token.tokenize)

text_counts = cv.fit_transform(df_slice['text'])

X_train, X_test, y_train, y_test = train_test_split(text_counts, y, test_size=0.2, random_state=42)

MNB = MultinomialNB()
MNB.fit(X_train, y_train)
y_pred = MNB.predict(X_test)

print(f"Multinomial Naive Bayes (MNB) with ngram_range={cv.ngram_range}")
print(f"Accuracy: {accuracy_score(y_test, y_pred)}")
print("\n")
print(classification_report(y_test, y_pred))
print("\n")
print(confusion_matrix(y_test, y_pred))

# let's try different n-grams
cv = CountVectorizer(stop_words='english',ngram_range = (3,3),tokenizer = token.tokenize)

text_counts = cv.fit_transform(df_slice['text'])

X_train, X_test, y_train, y_test = train_test_split(text_counts, y, test_size=0.2, random_state=42)

MNB = MultinomialNB()
MNB.fit(X_train, y_train)
y_pred = MNB.predict(X_test)

print(f"Multinomial Naive Bayes (MNB) with ngram_range={cv.ngram_range}")
print(f"Accuracy: {accuracy_score(y_test, y_pred)}")
print("\n")
print(classification_report(y_test, y_pred))
print("\n")
print(confusion_matrix(y_test, y_pred))

from sklearn.feature_extraction.text import TfidfVectorizer

tfidf = TfidfVectorizer()

text_tf = tfidf.fit_transform(df_slice['text'])

X_train, X_test, y_train, y_test = train_test_split(text_tf, y, test_size=0.2, random_state=42)

MNB = MultinomialNB()
MNB.fit(X_train, y_train)
y_pred = MNB.predict(X_test)

print(f"Multinomial Naive Bayes (MNB) with TfidfVectorizer")
print(f"Accuracy: {accuracy_score(y_test, y_pred)}")
print("\n")
print(classification_report(y_test, y_pred))
print("\n")
print(confusion_matrix(y_test, y_pred))

# %% Load and pre-process dataset, then save using pickle

# Define the path for the pickle file
PROCESSED_DATA_FILE = 'processed_data.pkl'

# Check if the processed data already exists
df_embeddings = load_from_pickle(PROCESSED_DATA_FILE)

# If the processed data does not exist, preprocess the data
if df_embeddings is None:
    df_embeddings = preprocess_data(SAMPLE_SIZE, PROCESSED_DATA_FILE)
    save_to_pickle(df_embeddings, PROCESSED_DATA_FILE)

# Show the first 5 rows of the processed dataset
print(df_embeddings.head())

X = []
for sentence in list(df_embeddings['text']):
    X.append(sentence)
# X just contains all the review texts (in Sentiment Analysis usually referred to as 'sentences')

y = list(df_embeddings['label'])
y = np.array(list(map(lambda x: 1 if x == 2 or x == '2' else 0, y)))
# y is an array of labels. We want it to be an np.array type in order for the validation split to be possible in fitting the model

sns.countplot(x=y)

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state=42)
# initial split

from tensorflow.keras.preprocessing.text import Tokenizer

tokenizer = Tokenizer()
tokenizer.fit_on_texts(X_train)

X_train = tokenizer.texts_to_sequences(X_train)
X_test = tokenizer.texts_to_sequences(X_test)
# turn input sentences into sequences of tokens

from tensorflow.keras.preprocessing.sequence import pad_sequences

vocab_size = len(tokenizer.word_index) + 1
# every key is another word in the vocabulary. The + 1 is because of reserving padding

max_length = 100

X_train = pad_sequences(X_train, padding = 'post', maxlen = max_length)
X_test = pad_sequences(X_test , padding = 'post', maxlen = max_length)

# requires embeddings to have already been computed by the LSTM code
EMBEDDINGS_MATRIX_FILE = 'embeddings_matrix.pkl'

embedding_matrix = load_from_pickle(EMBEDDINGS_MATRIX_FILE)

# turn X_train and X_test token sequences into embeddings

def to_embeddings(X, max_length, vocab_size, embedding_matrix):
    X_embeddings = np.zeros((len(X), max_length, 100))
    for i, sentence in enumerate(X):
        for j, token in enumerate(sentence):
            if token == 0:
                continue
            X_embeddings[i, j] = embedding_matrix[token]
    return X_embeddings

X_train = to_embeddings(X_train, max_length, vocab_size, embedding_matrix)
X_test = to_embeddings(X_test, max_length, vocab_size, embedding_matrix)
X_train.shape

# flatten the embeddings
X_train = X_train.reshape(X_train.shape[0], -1)
X_test = X_test.reshape(X_test.shape[0], -1)
X_train.shape

# normalize the embeddings with min-max scaling
from sklearn.preprocessing import MinMaxScaler

scaler = MinMaxScaler()
X_train = scaler.fit_transform(X_train)
X_test = scaler.transform(X_test)

# train a Naive Bayes classifier on the embeddings

from sklearn.naive_bayes import MultinomialNB

MNB = MultinomialNB()
MNB.fit(X_train, y_train)

y_pred = MNB.predict(X_test)

print(f"Multinomial Naive Bayes (MNB) with embeddings")
print(f"Accuracy: {accuracy_score(y_test, y_pred)}")
print("\n")
print(classification_report(y_test, y_pred))
print("\n")
print(confusion_matrix(y_test, y_pred))