# -*- coding: utf-8 -*-
"""sentiment_analysis_LSTM.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1CJzTePqE56EmWtdUfRzZqldPl6uEU9g5
"""

#from google.colab import drive
#drive.mount('/content/drive')
#
#!ls '/content/drive/MyDrive'

# Commented out IPython magic to ensure Python compatibility.
# %cd '/content/drive/MyDrive/EARIN_Project'

#!ls

import requests
import os

def download_file(url, filename):
    """
    Download a file from a given URL and save it to a given filename.
    If the file already exists, do nothing.
    """
    if not os.path.exists(filename):
        # send a GET request to the URL
        response = requests.get(url)
        # write the content of the response to a file
        with open(filename, 'wb') as file:
            file.write(response.content)

# Download the BERT vocabulary file if not already present
VOCAB_URL = "https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-vocab.txt"
VOCAB_FILE = "bert-base-uncased-vocab.txt"

download_file(VOCAB_URL, VOCAB_FILE)

import nltk
from nltk.corpus import stopwords

# Download necessary NLTK data
nltk.download('stopwords')

import pickle

def save_to_pickle(obj, filename):
    """
    Save an object to a pickle file.
    """
    print(f"Saving {filename} to pickle file...")
    with open(filename, 'wb') as file:
        pickle.dump(obj, file)

def load_from_pickle(filename):
    """
    Load an object from a pickle file if it exists.
    """
    if not os.path.exists(filename):
        return None
    print(f"Loading {filename} from pickle file...")
    with open(filename, 'rb') as file:
        return pickle.load(file)

#!pip install langdetect
#
#!pip install emoji
#
#!pip install --upgrade pandas

import bz2
import pandas as pd; print(pd.__version__)
from langdetect import detect, DetectorFactory
from langdetect.lang_detect_exception import LangDetectException
import re
import emoji

DetectorFactory.seed = 42

def preprocess_data(sample_size, filename):
    # Load and balance the dataset
    file_path = 'train.ft.txt.bz2'
    positive_reviews = []
    negative_reviews = []

    # we want to use a balanced dataset of sample reviews
    # we pick half negative and half positive reviews
    with bz2.open(file_path, 'rt', encoding='utf-8') as file:
        for line in file:
            label, text = line.split(' ', 1)
            label = int(label[-1])
            if label == 1 and len(negative_reviews) < sample_size // 2:
                negative_reviews.append([label, text])
            elif label == 2 and len(positive_reviews) < sample_size // 2:
                positive_reviews.append([label, text])
            if len(positive_reviews) == sample_size // 2 and len(negative_reviews) == sample_size // 2:
                break

    # Combine positive and negative reviews
    data = positive_reviews + negative_reviews
    df = pd.DataFrame(data, columns=['label', 'text'])

    # Function to detect language
    def detect_language(text):
        try:
            return detect(text)
        except LangDetectException:
            return "unknown"

    # Detect language
    df['language'] = df['text'].apply(detect_language)

    # Filter out non-English reviews
    df = df[df['language'] == 'en']
    df = df.drop(columns=['language'])

    # Initialize stopwords
    stop_words = set(stopwords.words('english'))

    def clean_text(text):
        # Replace emojis with descriptive words
        text = emoji.demojize(text, delimiters=(" ", " "))
        # Remove HTML tags
        text = re.sub(r'<.*?>', '', text)
        # Remove URLs
        text = re.sub(r'http\S+|www.\S+', '', text)
        # Remove hashtags
        text = re.sub(r'#\w+', '', text)
        # Remove special characters and digits
        text = re.sub(r'[^a-zA-Z\s]', '', text)
        # Convert to lowercase
        text = text.lower()
        # Remove stopwords
        text = ' '.join(word for word in text.split() if word not in stop_words)
        return text

    # Apply text cleaning to the 'text' column
    df['text'] = df['text'].apply(clean_text)

    # this is a rather simple way to shuffle a DataFrame
    df = df.sample(frac=1,random_state=42)

    return df

SAMPLE_SIZE = 20_000

# %% Load and pre-process dataset, then save using pickle

# Define the path for the pickle file
PROCESSED_DATA_FILE = 'processed_data.pkl'

# Check if the processed data already exists
df = load_from_pickle(PROCESSED_DATA_FILE)

# If the processed data does not exist, preprocess the data
if df is None:
    df = preprocess_data(SAMPLE_SIZE, PROCESSED_DATA_FILE)
    save_to_pickle(df, PROCESSED_DATA_FILE)

# Show the first 5 rows of the processed dataset
print(df.head())

import seaborn as sns

sns.countplot(x=df['label'])

import numpy as np; print(np.__version__)

#SLICE_SIZE = 5_000
#df_slice_1 = df[:SLICE_SIZE // 2].copy()
#df_slice_2 = df[df.shape[0] // 2: df.shape[0] // 2 + SLICE_SIZE // 2].copy()
#df_slice_2.index = np.arange(SLICE_SIZE // 2, SLICE_SIZE)
#df_slice = pd.concat([df_slice_1, df_slice_2])
df_slice = df # originally we did some tests with smaller DataFrame sizes. Now this cell is simply an alias for the same DataFrame

X = []
for sentence in list(df_slice['text']):
    X.append(sentence)
# X just contains all the review texts (in Sentiment Analysis usually referred to as 'sentences')

y = list(df_slice['label'])
y = np.array(list(map(lambda x: 1 if x == 2 or x == '2' else 0, y)))
# y is an array of labels. We want it to be an np.array type in order for the validation split to be possible in fitting the model

sns.countplot(x=y)

from sklearn.model_selection import train_test_split

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state=42)
# initial split

from tensorflow.keras.preprocessing.text import Tokenizer

tokenizer = Tokenizer()
tokenizer.fit_on_texts(X_train)

X_train = tokenizer.texts_to_sequences(X_train)
X_test = tokenizer.texts_to_sequences(X_test)
# turn input sentences into sequences of tokens

from tensorflow.keras.preprocessing.sequence import pad_sequences

import torch

device = torch.device("cuda:0" if torch.cuda.is_available() else "cpu")

device

vocab_size = len(tokenizer.word_index) + 1
# every key is another word in the vocabulary. The + 1 is because of reserving padding

max_length = 100

X_train = pad_sequences(X_train, padding = 'post', maxlen = max_length)
X_test = pad_sequences(X_test , padding = 'post', maxlen = max_length)

X_train = np.array(X_train)
X_test = np.array(X_test)
# turn then into np.array type so that the validation split in fitting works

EMBEDDINGS_MATRIX_FILE = 'embeddings_matrix.pkl'

embedding_matrix = load_from_pickle(EMBEDDINGS_MATRIX_FILE)

embeddings_dictionary = None
if embedding_matrix is None:
    embeddings_dictionary = dict()
    glove_file = open('glove.6B.100d.txt', encoding="utf8")

    for line in glove_file:
        records = line.split()
        word = records[0]
        vector_dimensions = np.asarray(records[1:], dtype='float32')
        embeddings_dictionary[word] = vector_dimensions
    glove_file.close()

if embedding_matrix is None:
    embedding_matrix = np.zeros((vocab_size, 100))
    for word, index in tokenizer.word_index.items():
        embedding_vector = embeddings_dictionary.get(word)
        if embedding_vector is not None:
            embedding_matrix[index] = embedding_vector

    save_to_pickle(embedding_matrix, EMBEDDINGS_MATRIX_FILE)

# embedding_matrix is 2D of shape (vocab_size, 100). Since we are using glove.6B.100d.txt
# for the embeddings each one of our words is associated with a weight vector of size 100

from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Embedding, LSTM, Dense, Dropout, Bidirectional, BatchNormalization

import tensorflow as tf
print("Num GPUs Available: ", len(tf.config.experimental.list_physical_devices('GPU')))

lstm_layer_sizes = (256, 128)

# If a GPU is available the keras model will automatically use it for computations

model = Sequential([
    # input the word tokens and generate the embeddings. The generated embeddings have size 100, as we explained earlier
    Embedding(vocab_size, 100, weights=[embedding_matrix], input_length=max_length, trainable=False),
    Dropout(0.2),
    # the bidirectional LSTM layer aims to capture information from both directions
    Bidirectional(LSTM(lstm_layer_sizes[0], return_sequences=True)),
    BatchNormalization(),
    Dropout(0.2),
    LSTM(lstm_layer_sizes[1]), # this LSTM layer is smaller than the first and aims to further process the temporal features
    # capturing higher-level temporal dependencies
    Dense(lstm_layer_sizes[1], activation='relu'), # this fully connected layer aims to
    # learn from the output of the LSTM layers.
    Dense(1, activation='sigmoid') # sigmoid helps giving a probability between 0 and 1
    # output is just 1 element
])

model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])
# as we have seen in previous laboratories the Adam optimizer is usually the best and fastest choice
# since it combines well most of the optimizing ideas
# the binary crossentropy as a loss function is typicall for classification problems

history = model.fit(X_train, y_train, batch_size=128, epochs=10, validation_split=0.2,verbose=1)
# batch_size: number of samples per gradient update
# epochs: number of epochs to train the model
# validation_split: fraction of the training data to be used as validation data.
# The model will set appart this fraction of the training data, will not train on it,
# and will evaluate the loss and any model metrics on this data at the end of each epoch.

model.save('model1.h5')

score = model.evaluate(X_test, y_test, verbose=1)

import matplotlib.pyplot as plt

plt.plot(history.history['accuracy'])
plt.plot(history.history['val_accuracy'])

plt.title(f'Model Accuracy for model {lstm_layer_sizes}')
plt.xlabel('epochs')
plt.ylabel('accuracy')
plt.legend(['train' , 'test'], loc = 'upper left')
plt.show()

plt.plot(history.history['loss'])
plt.plot(history.history['val_loss'])

plt.title(f'Model Loss for model {lstm_layer_sizes}')
plt.xlabel('epochs')
plt.ylabel('loss')
plt.legend(['train' , 'test'], loc = 'upper left')
plt.show()

# classification report and confusion matrix
from sklearn.metrics import classification_report, confusion_matrix

y_pred = model.predict(X_test)
y_pred = (y_pred > 0.5)

print(f'Metrics for model {lstm_layer_sizes}')
print(classification_report(y_test, y_pred))
print(confusion_matrix(y_test, y_pred))

lstm_layer_sizes = (128, 64)

# now let's do one with half the neurons

model_128 = Sequential([
    # input the word tokens and generate the embeddings. The generated embeddings have size 100, as we explained earlier
    Embedding(vocab_size, 100, weights=[embedding_matrix], input_length=max_length, trainable=False),
    Dropout(0.2),
    # the bidirectional LSTM layer aims to capture information from both directions
    Bidirectional(LSTM(lstm_layer_sizes[0], return_sequences=True)),
    BatchNormalization(),
    Dropout(0.2),
    LSTM(lstm_layer_sizes[1]), # this LSTM layer is smaller than the first and aims to further process the temporal features
    # capturing higher-level temporal dependencies
    Dense(lstm_layer_sizes[1], activation='relu'), # this fully connected layer aims to
    # learn from the output of the LSTM layers.
    Dense(1, activation='sigmoid') # sigmoid helps giving a probability between 0 and 1
    # output is just 1 element
])

model_128.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])
# as we have seen in previous laboratories the Adam optimizer is usually the best and fastest choice
# since it combines well most of the optimizing ideas
# the binary crossentropy as a loss function is typicall for classification problems

history = model_128.fit(X_train, y_train, batch_size=128, epochs=10, validation_split=0.2,verbose=1)

model_128.save('model128.h5')

score = model_128.evaluate(X_test, y_test, verbose=1)

plt.plot(history.history['accuracy'])
plt.plot(history.history['val_accuracy'])

plt.title(f'Model Accuracy for model {lstm_layer_sizes}')
plt.xlabel('epochs')
plt.ylabel('accuracy')
plt.legend(['train' , 'test'], loc = 'upper left')
plt.show()

plt.plot(history.history['loss'])
plt.plot(history.history['val_loss'])

plt.title(f'Model Loss for model {lstm_layer_sizes}')
plt.xlabel('epochs')
plt.ylabel('loss')
plt.legend(['train' , 'test'], loc = 'upper left')
plt.show()

y_pred = model_128.predict(X_test)
y_pred = (y_pred > 0.5)

print(f'Metrics for model {lstm_layer_sizes}')
print(classification_report(y_test, y_pred))
print(confusion_matrix(y_test, y_pred))

lstm_layer_sizes = (512, 128)

# and one with double the neurons

model_512 = Sequential([
    # input the word tokens and generate the embeddings. The generated embeddings have size 100, as we explained earlier
    Embedding(vocab_size, 100, weights=[embedding_matrix], input_length=max_length, trainable=False),
    Dropout(0.2),
    # the bidirectional LSTM layer aims to capture information from both directions
    Bidirectional(LSTM(lstm_layer_sizes[0], return_sequences=True)),
    BatchNormalization(),
    Dropout(0.2),
    LSTM(lstm_layer_sizes[1]), # this LSTM layer is smaller than the first and aims to further process the temporal features
    # capturing higher-level temporal dependencies
    Dense(lstm_layer_sizes[1], activation='relu'), # this fully connected layer aims to
    # learn from the output of the LSTM layers.
    Dense(1, activation='sigmoid') # sigmoid helps giving a probability between 0 and 1
    # output is just 1 element
])

model_512.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])
# as we have seen in previous laboratories the Adam optimizer is usually the best and fastest choice
# since it combines well most of the optimizing ideas
# the binary crossentropy as a loss function is typicall for classification problems

history = model_512.fit(X_train, y_train, batch_size=128, epochs=10, validation_split=0.2,verbose=1)

model_512.save('model512.h5')

score = model_512.evaluate(X_test, y_test, verbose=1)

plt.plot(history.history['accuracy'])
plt.plot(history.history['val_accuracy'])

plt.title(f'Model Accuracy for model {lstm_layer_sizes}')
plt.xlabel('epochs')
plt.ylabel('accuracy')
plt.legend(['train' , 'test'], loc = 'upper left')
plt.show()

plt.plot(history.history['loss'])
plt.plot(history.history['val_loss'])

plt.title(f'Model Loss for model {lstm_layer_sizes}')
plt.xlabel('epochs')
plt.ylabel('loss')
plt.legend(['train' , 'test'], loc = 'upper left')
plt.show()

y_pred = model_512.predict(X_test)
y_pred = (y_pred > 0.5)

print(f'Metrics for model {lstm_layer_sizes}')
print(classification_report(y_test, y_pred))
print(confusion_matrix(y_test, y_pred))

lstm_layer_sizes = (256, 64)

model_mix = Sequential([
    # input the word tokens and generate the embeddings. The generated embeddings have size 100, as we explained earlier
    Embedding(vocab_size, 100, weights=[embedding_matrix], input_length=max_length, trainable=False),
    Dropout(0.2),
    # the bidirectional LSTM layer aims to capture information from both directions
    Bidirectional(LSTM(lstm_layer_sizes[0], return_sequences=True)),
    BatchNormalization(),
    Dropout(0.2),
    LSTM(lstm_layer_sizes[1]), # this LSTM layer is smaller than the first and aims to further process the temporal features
    # capturing higher-level temporal dependencies
    Dense(lstm_layer_sizes[1], activation='relu'), # this fully connected layer aims to
    # learn from the output of the LSTM layers.
    Dense(1, activation='sigmoid') # sigmoid helps giving a probability between 0 and 1
    # output is just 1 element
])

model_mix.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])
# as we have seen in previous laboratories the Adam optimizer is usually the best and fastest choice
# since it combines well most of the optimizing ideas
# the binary crossentropy as a loss function is typicall for classification problems

history = model_mix.fit(X_train, y_train, batch_size=128, epochs=10, validation_split=0.2,verbose=1)

model_mix.save('modelmix.h5')

score = model_mix.evaluate(X_test, y_test, verbose=1)

plt.plot(history.history['accuracy'])
plt.plot(history.history['val_accuracy'])

plt.title(f'Model Accuracy for model {lstm_layer_sizes}')
plt.xlabel('epochs')
plt.ylabel('accuracy')
plt.legend(['train' , 'test'], loc = 'upper left')
plt.show()

plt.plot(history.history['loss'])
plt.plot(history.history['val_loss'])

plt.title(f'Model Loss for model {lstm_layer_sizes}')
plt.xlabel('epochs')
plt.ylabel('loss')
plt.legend(['train' , 'test'], loc = 'upper left')
plt.show()

y_pred = model_mix.predict(X_test)
y_pred = (y_pred > 0.5)

print(f'Metrics for model {lstm_layer_sizes}')
print(classification_report(y_test, y_pred))
print(confusion_matrix(y_test, y_pred))

